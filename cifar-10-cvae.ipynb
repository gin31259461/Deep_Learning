{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a conditional VAE on CIFAR-10 that can generate images of 10 classes\n",
    "\n",
    "[Benchmark](https://paperswithcode.com/sota/image-generation-on-cifar-10)\n",
    "\n",
    "Ref."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch_directml\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import FID\n",
    "import ignite.distributed as idist\n",
    "from ignite.handlers import ProgressBar\n",
    "from typing import Type\n",
    "from tqdm.autonotebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "for i in range(torch_directml.device_count()):\n",
    "    print(i, \":\", torch_directml.device_name(i))\n",
    "\n",
    "dml = torch_directml.device(0)\n",
    "print(\"dml =\", dml)\n",
    "\n",
    "device = dml\n",
    "kwargs = {\"num_workers\": 1, \"pin_memory\": True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "classes = training_data.classes\n",
    "class_size = len(classes)\n",
    "image_size = 32\n",
    "chanel_num = 3\n",
    "\n",
    "# Hyperparameters\n",
    "training = True\n",
    "activation = \"relu\"\n",
    "fc_output_features = 400\n",
    "latent_size = 20\n",
    "batch_size = 64\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.data = training_data.data.astype(\"uint8\")\n",
    "test_data.data = test_data.data.astype(\"uint8\")\n",
    "\n",
    "s1, s2 = random_split(training_data, [0.8, 0.2], torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(s1, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "validate_loader = DataLoader(s2, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cifar10_images(images, labels, n):\n",
    "    plt.figure(figsize=[10, 10])\n",
    "\n",
    "    for i in range(n):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[i])\n",
    "        plt.xlabel(classes[labels[i]])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cifar10_images(training_data.data, training_data.targets, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training images shape: \", training_data.data.shape)\n",
    "print(\"Test images shape: \", test_data.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "# training_data.data = training_data.data.astype(\"float32\") / 255.0\n",
    "# test_data.data = test_data.data.astype(\"float32\") / 255.0\n",
    "\n",
    "# training_data.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build cVAE model\n",
    "\n",
    "[cVAE mechanism](https://idiotdeveloper.com/introduction-to-autoencoders/)\n",
    "\n",
    "![cVAE mechanism](./images/variational-autoencoder.png)\n",
    "\n",
    "Ref.\n",
    "\n",
    "[Understanding Conditional Variational Autoencoders](https://towardsdatascience.com/understanding-conditional-variational-autoencoders-cd62b4f57bf8)\n",
    "\n",
    "[Conditional Variational Autoencoder (cVAE) using PyTorch](https://github.com/unnir/cVAE)\n",
    "\n",
    "[Conditional Variational Autoencoder in Keras](https://github.com/nnormandin/Conditional_VAE/blob/master/Conditional_VAE.ipynb)\n",
    "\n",
    "[GAN Evaluation : the Frechet Inception Distance and Inception Score metrics](https://colab.research.google.com/github/pytorch-ignite/pytorch-ignite.ai/blob/gh-pages/blog/2021-08-11-GAN-evaluation-using-FID-and-IS.ipynb#scrollTo=Stp59yfH65VO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    需要以下三個結構\n",
    "    1. encoder\n",
    "    2. reparameterizer\n",
    "    3. decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_size, latent_size, class_size, *args, **kwargs) -> None:\n",
    "        super(CVAE, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.feature_size = feature_size\n",
    "        self.class_size = class_size\n",
    "\n",
    "        # encoder\n",
    "        self.fc1 = nn.Linear(feature_size + class_size, fc_output_features)\n",
    "        self.fc21 = nn.Linear(fc_output_features, latent_size)\n",
    "        self.fc22 = nn.Linear(fc_output_features, latent_size)\n",
    "\n",
    "        # decoder\n",
    "        self.fc3 = nn.Linear(latent_size + class_size, fc_output_features)\n",
    "        self.fc4 = nn.Linear(fc_output_features, feature_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x, c):  # Q(z|x, c)\n",
    "        \"\"\"\n",
    "        x: (bs, feature_size)\n",
    "        c: (bs, class_size)\n",
    "        \"\"\"\n",
    "        inputs = torch.cat([x, c], 1)  # (bs, feature_size + class_size)\n",
    "        h1 = self.relu(self.fc1(inputs))\n",
    "        z_mu = self.fc21(h1)\n",
    "        z_var = self.fc22(h1)\n",
    "        return z_mu, z_var\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):  # P(x|z, c)\n",
    "        \"\"\"\n",
    "        z: (bs, latent_size)\n",
    "        c: (bs, class_size)\n",
    "        \"\"\"\n",
    "        inputs = torch.cat([z, c], 1)  # (bs, latent_size + class_size)\n",
    "        h3 = self.relu(self.fc3(inputs))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x.view(-1, self.feature_size), c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z, c), mu, logvar\n",
    "\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, image_size * image_size * chanel_num), reduction=\"sum\")\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "model = CVAE(image_size * image_size * chanel_num, latent_size, class_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "def one_hot(labels, class_size):\n",
    "    targets = torch.zeros(labels.size(0), class_size)\n",
    "    for i, label in enumerate(labels):\n",
    "        targets[i, label] = 1\n",
    "    return targets.to(device)\n",
    "\n",
    "\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "fid_scores = []\n",
    "\n",
    "\n",
    "def train(epoch, data_loader: Type[DataLoader]):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data, labels) in enumerate(data_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        labels = one_hot(labels, class_size)\n",
    "        recon_batch, mu, logvar = model(data, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.detach().cpu().numpy()\n",
    "        optimizer.step()\n",
    "\n",
    "        # log\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(data_loader.dataset),\n",
    "                    100.0 * batch_idx / len(data_loader),\n",
    "                    loss.item() / len(data),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    train_loss /= len(data_loader.dataset)\n",
    "    train_loss_history.append(train_loss)\n",
    "    print(\"====> Epoch: {} Average loss: {:.4f}\".format(epoch, train_loss))\n",
    "\n",
    "\n",
    "def test(epoch, data_loader: Type[DataLoader]):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, labels) in enumerate(data_loader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            labels = one_hot(labels, class_size)\n",
    "            recon_batch, mu, logvar = model(data, labels)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).detach().cpu().numpy()\n",
    "\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 5)\n",
    "                comparison = torch.cat([data[:n], recon_batch.view(-1, chanel_num, image_size, image_size)[:n]])\n",
    "                save_image(\n",
    "                    comparison.cpu(),\n",
    "                    \"./outputs/cifar-10-cvae-outputs/temp/reconstruction_\" + str(f\"{epoch:02}\") + \".png\",\n",
    "                    nrow=n,\n",
    "                )\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    test_loss_history.append(test_loss)\n",
    "    print(\"====> Test set loss: {:.4f}\".format(test_loss))\n",
    "\n",
    "\n",
    "def interpolate(batch):\n",
    "    arr = []\n",
    "    for img in batch:\n",
    "        pil_img = transforms.ToPILImage()(img)\n",
    "        resized_img = pil_img.resize((299, 299), Image.BILINEAR)\n",
    "        arr.append(transforms.ToTensor()(resized_img))\n",
    "    return torch.stack(arr).reshape((len(batch), 3, 299, 299))\n",
    "\n",
    "\n",
    "def evaluation(engine: Engine, batch_data):\n",
    "    model.eval()\n",
    "\n",
    "    data = batch_data[0]\n",
    "    labels = batch_data[1]\n",
    "\n",
    "    data, labels = data.to(device), labels.to(device)\n",
    "    labels = one_hot(labels, class_size)\n",
    "    recon_batch, mu, logvar = model(data, labels)\n",
    "\n",
    "    n = min(data.size(0), 5)\n",
    "\n",
    "    fake = interpolate(recon_batch.view(-1, chanel_num, image_size, image_size)[:n])\n",
    "    real = interpolate(data[:n])\n",
    "\n",
    "    return fake, real\n",
    "\n",
    "\n",
    "evaluator = Engine(evaluation)\n",
    "fid_metric = FID(device=idist.device())\n",
    "fid_metric.attach(evaluator, \"fid\")\n",
    "\n",
    "# ProgressBar().attach(evaluator)\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch, train_loader)\n",
    "    test(epoch, validate_loader if training else test_loader)\n",
    "    evaluator.run(validate_loader if training else test_loader, max_epochs=1)\n",
    "\n",
    "    metrics = evaluator.state.metrics\n",
    "    fid_score = metrics[\"fid\"]\n",
    "    fid_scores.append(fid_score)\n",
    "\n",
    "    print(\"====> FID score: {:.4f}\".format(fid_score))\n",
    "\n",
    "    # 在每個 epoch 測試: 用隨機取樣產生新圖片並輸出\n",
    "    with torch.no_grad():\n",
    "        c = torch.eye(class_size, class_size).to(device)\n",
    "        sample = torch.randn(class_size, latent_size).to(device)\n",
    "        sample = model.decode(sample, c).cpu()\n",
    "        images = sample.view(class_size, chanel_num, image_size, image_size)\n",
    "        save_image(\n",
    "            images,\n",
    "            \"./outputs/cifar-10-cvae-outputs/temp/sample_\" + str(f\"{epoch:02}\") + \".png\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training:\n",
    "    plt.figure(figsize=[6, 4])\n",
    "    plt.plot(train_loss_history, \"black\", linewidth=2.0)\n",
    "    plt.plot(test_loss_history, \"green\", linewidth=2.0)\n",
    "    plt.legend([\"Training Loss\", \"Validation Loss\"], fontsize=14)\n",
    "    plt.xlabel(\"Epochs\", fontsize=10)\n",
    "    plt.ylabel(\"Loss\", fontsize=10)\n",
    "    plt.title(\"Loss Curves\", fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=[6, 4])\n",
    "    plt.plot(fid_scores, \"red\", linewidth=2.0)\n",
    "    # plt.legend([\"Training Loss\", \"Validation Loss\"], fontsize=14)\n",
    "    plt.xlabel(\"Epochs\", fontsize=10)\n",
    "    plt.ylabel(\"FID\", fontsize=10)\n",
    "    plt.title(\"FID score\", fontsize=12)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
