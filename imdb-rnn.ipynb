{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using the IMDb reviews dataset - RNN-based model with attention/transformers mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref.\n",
    "\n",
    "Kaggle\n",
    "\n",
    "[Sentiment Analysis of IMDB Movie Reviews - gold](https://www.kaggle.com/code/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews)\n",
    "\n",
    "[Sentiment Analysis of IMDB Movie Reviews - cooper](https://www.kaggle.com/code/bhavikjikadara/sentiment-analysis-of-imdb-movie-reviews)\n",
    "\n",
    "[IMDB Dataset Sentiment Analysis using RNN](https://www.kaggle.com/code/tanyildizderya/imdb-dataset-sentiment-analysis-using-rnn)\n",
    "\n",
    "Keras\n",
    "\n",
    "[Keras - IMDB movie review sentiment classification dataset](https://keras.io/api/datasets/imdb/)\n",
    "\n",
    "這是一個包含 25,000 部電影評論的 IMDB 數據集，按情感（正面/負面）進行標記。評論已經過預處理，每個評論都被編碼為一個詞索引列表（整數）。為了方便起見，詞彙按照數據集中的整體頻率進行索引，因此例如整數 \"3\" 編碼了數據中第三個最常見的詞。這使得可以快速進行過濾操作，例如：\"只考慮前 10,000 個最常見的詞，但排除前 20 個最常見的詞\"。\n",
    "\n",
    "按照慣例，\"0\" 不代表特定的詞，而是用於編碼填充標記。\n",
    "\n",
    "Benchmark\n",
    "\n",
    "[Sentiment Analysis on IMDb](https://paperswithcode.com/sota/sentiment-analysis-on-imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python version: 3.10.6\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras import layers, losses, optimizers, regularizers\n",
    "from keras.utils import pad_sequences\n",
    "from keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence preprocessing parameters\n",
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 3\n",
    "max_num_words = 5000\n",
    "max_sequence_length = 512\n",
    "\n",
    "# training parameters\n",
    "training = True\n",
    "use_cuDNN = True\n",
    "embedding_output_dim = 64\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "validation_split = 0.1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
    "    num_words=max_num_words,\n",
    "    skip_top=0,\n",
    "    maxlen=None,\n",
    "    seed=113,\n",
    "    start_char=start_char,\n",
    "    oov_char=oov_char,\n",
    "    index_from=index_from,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data shape:\", x_train.shape)\n",
    "print(\"Training label shape:\", y_train.shape)\n",
    "print(\"Test data shape:\", x_test.shape)\n",
    "print(\"Test label shape:\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"y train distribution: \", dict(zip(unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(\"y test distribution: \", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_decoded_sequence(data, index=0):\n",
    "    # 取得字典 mapping => { word: index ...}\n",
    "    word_index = imdb.get_word_index()\n",
    "\n",
    "    # key, value => word, index\n",
    "    inverted_word_index = dict((index + index_from, word) for (word, index) in word_index.items())\n",
    "\n",
    "    # Update `inverted_word_index` to include `start_char` and `oov_char`\n",
    "    inverted_word_index[0] = \"[MASK]\"\n",
    "    inverted_word_index[start_char] = \"[START]\"\n",
    "    inverted_word_index[oov_char] = \"[OOV]\"\n",
    "\n",
    "    # X data (word sequence)\n",
    "\n",
    "    print(data[index])\n",
    "    decoded_sequence = \" \".join(inverted_word_index[i] for i in data[index])\n",
    "    print(decoded_sequence)\n",
    "\n",
    "    # y data (labels: positive or negative)\n",
    "\n",
    "    print(y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_decoded_sequence(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training label distribution\n",
    "\n",
    "plt.figure()\n",
    "sns.countplot(pd.DataFrame(y_train, columns=[\"class\"]), x=\"class\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"y train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test label distribution\n",
    "\n",
    "plt.figure()\n",
    "sns.countplot(pd.DataFrame(y_test, columns=[\"class\"]), x=\"class\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"y test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words distribution\n",
    "\n",
    "\n",
    "def visualize_sequence_distribution(x_train, x_test):\n",
    "    review_len_train = []\n",
    "    review_len_test = []\n",
    "    for i, j in zip(x_train, x_test):\n",
    "        review_len_train.append(len(i))\n",
    "        review_len_test.append(len(j))\n",
    "\n",
    "    print(\"min:\", min(review_len_train), \"max:\", max(review_len_train))\n",
    "    print(\"min:\", min(review_len_test), \"max:\", max(review_len_test))\n",
    "\n",
    "    sns.displot(review_len_train, rug_kws={\"alpha\": 0.3})\n",
    "    plt.xlabel(\"review length\")\n",
    "    plt.title(\"review train\")\n",
    "    sns.displot(review_len_test, rug_kws={\"alpha\": 0.3})\n",
    "    plt.xlabel(\"review length\")\n",
    "    plt.title(\"review test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sequence_distribution(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sequence_len = np.mean([len(seq) for seq in x_train])\n",
    "mean_sequence_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras's IMDB\n",
    "\n",
    "X data : 資料已經預處理過，包括\n",
    "* normalization => setting English stopwords\n",
    "* removing html strips and noise text\n",
    "* removing special characters\n",
    "* segmentation (斷詞)\n",
    "* removing stopwords\n",
    "* encoding (編碼)\n",
    "\n",
    "y data : 代表正向 (positive) 或負向 (negative) 的評論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使每個 sequence 有相同的長度\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_sequence_length, padding=\"pre\")\n",
    "x_test = pad_sequences(x_test, maxlen=max_sequence_length, padding=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data shape:\", x_train.shape)\n",
    "print(\"Training label shape:\", y_train.shape)\n",
    "print(\"Test data shape:\", x_test.shape)\n",
    "print(\"Test label shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sequence_distribution(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_decoded_sequence(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN mechanism\n",
    "\n",
    "Ref.\n",
    "\n",
    "[一文搞懂RNN（循环神经网络）基础篇](https://zhuanlan.zhihu.com/p/30844905)\n",
    "\n",
    "![rnn](./images/rnn.png)\n",
    "\n",
    "S<sub>t</sub> 代表神經元在 t 時刻上，同時接收\n",
    "* 輸入 X 與其輸入權重 U\n",
    "* 前一次 S 在 t-1 時刻的 S<sub>t-1</sub> 與其 W (S<sub>t-1</sub> --> S<sub>t</sub> 的權重)\n",
    "\n",
    "展開\n",
    "\n",
    "![rnn-expand](./images/rnn-expand.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mechanism\n",
    "\n",
    "Ref.\n",
    "\n",
    "[完全解析RNN, Seq2Seq, Attention注意力机制](https://zhuanlan.zhihu.com/p/51383402)\n",
    "\n",
    "[A simple overview of RNN, LSTM and Attention Mechanism](https://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b)\n",
    "\n",
    "[注意力機制 (Attention Mechanism) 的理解與實作](https://www.kaggle.com/code/lianghsunhuang/attention-mechanism)\n",
    "\n",
    "Attention 的架構\n",
    "\n",
    "![attention-mechanism](./images/attention-mechanism.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref.\n",
    "\n",
    "[Keras 中的循环神经网络 (RNN)](https://tensorflow.google.cn/guide/keras/rnn?hl=zh-cn)\n",
    "\n",
    "[Keras实现CNN、RNN（基于attention 的双向RNN）及两者的融合](https://blog.csdn.net/xwd18280820053/article/details/80060544)\n",
    "\n",
    "[Text classification with an RNN](https://www.tensorflow.org/text/tutorials/text_classification_rnn)\n",
    "\n",
    "[用LSTM模型分類IMDB電影資料集評論](https://dysonma.github.io/2020/11/21/LSTM_IMDB/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_activation = \"selu\" if not use_cuDNN else \"tanh\"\n",
    "\n",
    "\n",
    "class BidirectionalRnn(keras.Model):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        super(BidirectionalRnn, self).__init__()\n",
    "\n",
    "        self.embedding_1 = layers.Embedding(max_num_words, embedding_output_dim, mask_zero=False)\n",
    "        self.first_rnn = layers.Bidirectional(\n",
    "            layers.LSTM(\n",
    "                64,\n",
    "                activation=rnn_activation,\n",
    "                kernel_regularizer=regularizers.L2(0.01),\n",
    "                return_sequences=True,\n",
    "                dropout=0.1,\n",
    "            ),\n",
    "        )\n",
    "        self.second_rnn = layers.Bidirectional(\n",
    "            layers.LSTM(\n",
    "                64,\n",
    "                activation=rnn_activation,\n",
    "                kernel_regularizer=regularizers.L2(0.01),\n",
    "                return_sequences=False,\n",
    "                dropout=0.1,\n",
    "            ),\n",
    "        )\n",
    "        self.dense_1 = layers.Dense(64, activation=\"relu\")\n",
    "        self.final_output = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding_1(inputs, training=training)\n",
    "        x = self.first_rnn(x, training=training)\n",
    "        x = self.second_rnn(x, training=training)\n",
    "        x = self.dense_1(x, training=training)\n",
    "        x = self.final_output(x, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = BidirectionalRnn()\n",
    "rnn.build((None, max_sequence_length))\n",
    "rnn.call(layers.Input((max_sequence_length)))\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=False),\n",
    "    optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = rnn.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=num_epochs,\n",
    "    verbose=0,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=validation_split,\n",
    "    callbacks=[TqdmCallback(verbose=0)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training:\n",
    "    plt.figure(figsize=[6, 4])\n",
    "    plt.plot(train_history.history[\"loss\"], \"black\", linewidth=2.0)\n",
    "    plt.plot(train_history.history[\"val_loss\"], \"green\", linewidth=2.0)\n",
    "    plt.legend([\"Training Loss\", \"Validation Loss\"], fontsize=14)\n",
    "    plt.xlabel(\"Epochs\", fontsize=10)\n",
    "    plt.ylabel(\"Loss\", fontsize=10)\n",
    "    plt.title(\"Loss Curves\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training:\n",
    "    plt.figure(figsize=[6, 4])\n",
    "    plt.plot(train_history.history[\"accuracy\"], \"black\", linewidth=2.0)\n",
    "    plt.plot(train_history.history[\"val_accuracy\"], \"blue\", linewidth=2.0)\n",
    "    plt.legend([\"Training Accuracy\", \"Validation Accuracy\"], fontsize=14)\n",
    "    plt.xlabel(\"Epochs\", fontsize=10)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=10)\n",
    "    plt.title(\"Accuracy Curves\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not training:\n",
    "    test_result = rnn.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test result insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
