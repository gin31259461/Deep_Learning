{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using the IMDb reviews dataset - RNN-based model with attention/transformers mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref.\n",
    "\n",
    "Kaggle\n",
    "\n",
    "[Sentiment Analysis of IMDB Movie Reviews - gold](https://www.kaggle.com/code/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews)\n",
    "\n",
    "[Sentiment Analysis of IMDB Movie Reviews - cooper](https://www.kaggle.com/code/bhavikjikadara/sentiment-analysis-of-imdb-movie-reviews)\n",
    "\n",
    "[IMDB Dataset Sentiment Analysis using RNN](https://www.kaggle.com/code/tanyildizderya/imdb-dataset-sentiment-analysis-using-rnn)\n",
    "\n",
    "Keras\n",
    "\n",
    "[Keras - IMDB movie review sentiment classification dataset](https://keras.io/api/datasets/imdb/)\n",
    "\n",
    "包含 25,000 部電影評論的 IMDB 數據集，按情感（正面/負面）進行標記。評論已經過預處理，每個評論都被編碼為一個詞索引列表（整數），詞彙按照數據集中的整體頻率進行索引，所以可以快速篩選特定頻率區間的詞彙。\n",
    "\n",
    "按照慣例，\"0\" 不代表特定的詞，而是用於編碼填充標記。\n",
    "\n",
    "Benchmark\n",
    "\n",
    "[Sentiment Analysis on IMDb](https://paperswithcode.com/sota/sentiment-analysis-on-imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python version: 3.10.6\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras import layers, losses, optimizers\n",
    "from keras.utils import pad_sequences, plot_model\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"final\"\n",
    "\n",
    "# sequence preprocessing parameters\n",
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 3\n",
    "max_num_words = 5001\n",
    "max_sequence_length = 240\n",
    "\n",
    "# training parameters\n",
    "training = False\n",
    "use_cuDNN = True\n",
    "embedding_output_dim = 64\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "validation_split = 0.1 if training else 0.0\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
    "    num_words=max_num_words,\n",
    "    skip_top=0,\n",
    "    maxlen=None,\n",
    "    seed=113,\n",
    "    start_char=start_char,\n",
    "    oov_char=oov_char,\n",
    "    index_from=index_from,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data shape:\", x_train.shape)\n",
    "print(\"Training label shape:\", y_train.shape)\n",
    "print(\"Test data shape:\", x_test.shape)\n",
    "print(\"Test label shape:\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"y train distribution: \", dict(zip(unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(\"y test distribution: \", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得字典 mapping => { word: index ...}\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# key, value => word, index\n",
    "inverted_word_index = dict((index + index_from, word) for (word, index) in word_index.items())\n",
    "\n",
    "# Update `inverted_word_index` to include `start_char` and `oov_char`\n",
    "inverted_word_index[0] = \"[MASK]\"\n",
    "inverted_word_index[start_char] = \"[START]\"\n",
    "inverted_word_index[oov_char] = \"[OOV]\"\n",
    "\n",
    "\n",
    "def print_decoded_sequence(data, index=0):\n",
    "\n",
    "    # X data (word sequence)\n",
    "\n",
    "    print(np.array(data[index]))\n",
    "    decoded_sequence = \" \".join(inverted_word_index[i] for i in data[index])\n",
    "    print(decoded_sequence)\n",
    "    print(np.array(decoded_sequence.split(\" \")))\n",
    "\n",
    "    # y data (labels: positive or negative)\n",
    "\n",
    "    print(y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if max_num_words is None:\n",
    "    max_num_words = len(imdb.get_word_index().items())\n",
    "\n",
    "print(max_num_words)\n",
    "\n",
    "print_decoded_sequence(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training label distribution\n",
    "\n",
    "plt.figure()\n",
    "sns.countplot(pd.DataFrame(y_train, columns=[\"class\"]), x=\"class\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"y train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test label distribution\n",
    "\n",
    "plt.figure()\n",
    "sns.countplot(pd.DataFrame(y_test, columns=[\"class\"]), x=\"class\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"y test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words distribution\n",
    "\n",
    "\n",
    "def visualize_sequence_distribution(x_train, x_test):\n",
    "    review_len_train = []\n",
    "    review_len_test = []\n",
    "    for i, j in zip(x_train, x_test):\n",
    "        review_len_train.append(len(i))\n",
    "        review_len_test.append(len(j))\n",
    "\n",
    "    print(\"min:\", min(review_len_train), \"max:\", max(review_len_train))\n",
    "    print(\"min:\", min(review_len_test), \"max:\", max(review_len_test))\n",
    "\n",
    "    sns.displot(review_len_train, rug_kws={\"alpha\": 0.3})\n",
    "    plt.xlabel(\"review length\")\n",
    "    plt.title(\"review train\")\n",
    "    sns.displot(review_len_test, rug_kws={\"alpha\": 0.3})\n",
    "    plt.xlabel(\"review length\")\n",
    "    plt.title(\"review test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sequence_distribution(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sequence_len = np.mean([len(seq) for seq in x_train])\n",
    "mean_sequence_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras's IMDB\n",
    "\n",
    "X data : 資料已經預處理過，包括\n",
    "* removing html strips\n",
    "* removing special characters\n",
    "* segmentation\n",
    "* encoding\n",
    "\n",
    "y data : 代表正向 (positive) 或負向 (negative) 的評論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words.add(\"[START]\")\n",
    "stop_words.add(\"[OOV]\")\n",
    "\n",
    "\n",
    "def remove_stop_words(sequences):\n",
    "    filterd_sequences = []\n",
    "\n",
    "    for sequence in sequences:\n",
    "        filterd_sequences.append(\n",
    "            [word_index for word_index in sequence if inverted_word_index[word_index] not in stop_words],\n",
    "        )\n",
    "\n",
    "    return filterd_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "x_train = remove_stop_words(x_train.tolist())\n",
    "x_test = remove_stop_words(x_test.tolist())\n",
    "\n",
    "# 使每個 sequence 有相同的長度\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_sequence_length, padding=\"pre\")\n",
    "x_test = pad_sequences(x_test, maxlen=max_sequence_length, padding=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data shape:\", x_train.shape)\n",
    "print(\"Training label shape:\", y_train.shape)\n",
    "print(\"Test data shape:\", x_test.shape)\n",
    "print(\"Test label shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sequence_distribution(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_decoded_sequence(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN mechanism\n",
    "\n",
    "Ref.\n",
    "\n",
    "[一文搞懂RNN（循环神经网络）基础篇](https://zhuanlan.zhihu.com/p/30844905)\n",
    "\n",
    "![rnn](./images/rnn.png)\n",
    "\n",
    "S<sub>t</sub> 代表神經元在 t 時刻上，同時接收\n",
    "* 輸入 X 與其輸入權重 U\n",
    "* 前一次 S 在 t-1 時刻的 S<sub>t-1</sub> 與其 W (S<sub>t-1</sub> --> S<sub>t</sub> 的權重)\n",
    "\n",
    "展開\n",
    "\n",
    "![rnn-expand](./images/rnn-expand.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mechanism\n",
    "\n",
    "Ref.\n",
    "\n",
    "[完全解析RNN, Seq2Seq, Attention注意力机制](https://zhuanlan.zhihu.com/p/51383402)\n",
    "\n",
    "[A simple overview of RNN, LSTM and Attention Mechanism](https://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b)\n",
    "\n",
    "[注意力機制 (Attention Mechanism) 的理解與實作](https://www.kaggle.com/code/lianghsunhuang/attention-mechanism)\n",
    "\n",
    "Attention 的架構\n",
    "\n",
    "![attention-mechanism](./images/attention-mechanism.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref.\n",
    "\n",
    "[Keras 中的循环神经网络 (RNN)](https://tensorflow.google.cn/guide/keras/rnn?hl=zh-cn)\n",
    "\n",
    "[Keras实现CNN、RNN（基于attention 的双向RNN）及两者的融合](https://blog.csdn.net/xwd18280820053/article/details/80060544)\n",
    "\n",
    "[Text classification with an RNN](https://www.tensorflow.org/text/tutorials/text_classification_rnn)\n",
    "\n",
    "[How to add attention layer to a Bi-LSTM](https://stackoverflow.com/questions/62948332/how-to-add-attention-layer-to-a-bi-lstm/62949137#62949137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(layers.Layer):\n",
    "\n",
    "    def __init__(self, return_sequences=True):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
    "\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "\n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "\n",
    "        return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_activation = \"selu\" if not use_cuDNN else \"tanh\"\n",
    "input_shape = max_sequence_length\n",
    "\n",
    "input = layers.Input(shape=input_shape)\n",
    "\n",
    "x = layers.Embedding(max_num_words, embedding_output_dim, input_length=max_sequence_length, mask_zero=True)(input)\n",
    "\n",
    "x = layers.Bidirectional(layers.LSTM(64, activation=rnn_activation, return_sequences=True, dropout=0.5))(x)\n",
    "\n",
    "a = Attention(return_sequences=True)(x)\n",
    "\n",
    "p1 = layers.GlobalAveragePooling1D()(x)\n",
    "p2 = layers.GlobalAveragePooling1D()(a)\n",
    "\n",
    "merge = layers.Concatenate()([p1, p2])\n",
    "\n",
    "merge = layers.Dropout(0.5)(merge)\n",
    "\n",
    "x = layers.Dense(64, activation=\"relu\")(merge)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build((None, max_sequence_length))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=num_epochs,\n",
    "    # verbose=0,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=validation_split,\n",
    "    # callbacks=[TqdmCallback(verbose=0)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training:\n",
    "    plt.figure(figsize=[6, 4])\n",
    "    plt.plot(train_history.history[\"loss\"], \"black\", linewidth=2.0)\n",
    "    plt.plot(train_history.history[\"val_loss\"], \"green\", linewidth=2.0)\n",
    "    plt.legend([\"Training Loss\", \"Validation Loss\"], fontsize=14)\n",
    "    plt.xlabel(\"Epochs\", fontsize=10)\n",
    "    plt.ylabel(\"Loss\", fontsize=10)\n",
    "    plt.title(\"Loss Curves\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training:\n",
    "    plt.figure(figsize=[6, 4])\n",
    "    plt.plot(train_history.history[\"accuracy\"], \"black\", linewidth=2.0)\n",
    "    plt.plot(train_history.history[\"val_accuracy\"], \"blue\", linewidth=2.0)\n",
    "    plt.legend([\"Training Accuracy\", \"Validation Accuracy\"], fontsize=14)\n",
    "    plt.xlabel(\"Epochs\", fontsize=10)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=10)\n",
    "    plt.title(\"Accuracy Curves\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not training:\n",
    "    test_result = model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.models.save_model(model, \"./models/imdb-rnn-{0}\".format(version), save_format=\"tf\")\n",
    "\n",
    "# with open(\"./models/imdb-rnn-{0}-history\".format(version), \"wb\") as f:\n",
    "#     pickle.dump(train_history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not training, \"Training mode can't do final prediction\"\n",
    "\n",
    "# model = keras.models.load_model(\"./models/imdb-rnn-{0}\".format(version))\n",
    "\n",
    "pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test result insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_metrics = metrics.confusion_matrix(y_test, np.argmax(pred, axis=1))\n",
    "display = metrics.ConfusionMatrixDisplay(confusion_metrics)\n",
    "fig, axes = plt.subplots(figsize=(20, 10))\n",
    "display.plot(ax=axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, np.argmax(pred, axis=1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
