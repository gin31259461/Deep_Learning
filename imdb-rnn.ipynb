{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using the IMDb reviews dataset - RNN-based model with attention/transformers mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref.\n",
    "\n",
    "Kaggle\n",
    "\n",
    "[Sentiment Analysis of IMDB Movie Reviews - gold](https://www.kaggle.com/code/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews)\n",
    "\n",
    "[Sentiment Analysis of IMDB Movie Reviews - cooper](https://www.kaggle.com/code/bhavikjikadara/sentiment-analysis-of-imdb-movie-reviews)\n",
    "\n",
    "[IMDB Dataset Sentiment Analysis using RNN](https://www.kaggle.com/code/tanyildizderya/imdb-dataset-sentiment-analysis-using-rnn)\n",
    "\n",
    "Keras\n",
    "\n",
    "[Keras - IMDB movie review sentiment classification dataset](https://keras.io/api/datasets/imdb/)\n",
    "\n",
    "這是一個包含 25,000 部電影評論的 IMDB 數據集，按情感（正面/負面）進行標記。評論已經過預處理，每個評論都被編碼為一個詞索引列表（整數）。為了方便起見，詞彙按照數據集中的整體頻率進行索引，因此例如整數 \"3\" 編碼了數據中第三個最常見的詞。這使得可以快速進行過濾操作，例如：\"只考慮前 10,000 個最常見的詞，但排除前 20 個最常見的詞\"。\n",
    "\n",
    "按照慣例，\"0\" 不代表特定的詞，而是用於編碼填充標記。\n",
    "\n",
    "Benchmark\n",
    "\n",
    "[Sentiment Analysis on IMDb](https://paperswithcode.com/sota/sentiment-analysis-on-imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python version: 3.10.6\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras import layers, losses, optimizers, regularizers\n",
    "from keras.utils import pad_sequences, plot_model\n",
    "from keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence preprocessing parameters\n",
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 3\n",
    "max_num_words = 5000\n",
    "max_sequence_length = 240\n",
    "\n",
    "# training parameters\n",
    "training = True\n",
    "use_cuDNN = True\n",
    "embedding_output_dim = 64\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "validation_split = 0.1\n",
    "validation_steps = 30\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
    "    num_words=max_num_words,\n",
    "    skip_top=0,\n",
    "    maxlen=None,\n",
    "    seed=113,\n",
    "    start_char=start_char,\n",
    "    oov_char=oov_char,\n",
    "    index_from=index_from,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (25000,)\n",
      "Training label shape: (25000,)\n",
      "Test data shape: (25000,)\n",
      "Test label shape: [0 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data shape:\", x_train.shape)\n",
    "print(\"Training label shape:\", y_train.shape)\n",
    "print(\"Test data shape:\", x_test.shape)\n",
    "print(\"Test label shape:\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train distribution:  {0: 12500, 1: 12500}\n",
      "y test distribution:  {0: 12500, 1: 12500}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"y train distribution: \", dict(zip(unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(\"y test distribution: \", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_decoded_sequence(data, index=0):\n",
    "    # 取得字典 mapping => { word: index ...}\n",
    "    word_index = imdb.get_word_index()\n",
    "\n",
    "    # key, value => word, index\n",
    "    inverted_word_index = dict((index + index_from, word) for (word, index) in word_index.items())\n",
    "\n",
    "    # Update `inverted_word_index` to include `start_char` and `oov_char`\n",
    "    inverted_word_index[0] = \"[MASK]\"\n",
    "    inverted_word_index[start_char] = \"[START]\"\n",
    "    inverted_word_index[oov_char] = \"[OOV]\"\n",
    "\n",
    "    # X data (word sequence)\n",
    "\n",
    "    print(data[index])\n",
    "    decoded_sequence = \" \".join(inverted_word_index[i] for i in data[index])\n",
    "    print(decoded_sequence)\n",
    "\n",
    "    # y data (labels: positive or negative)\n",
    "\n",
    "    print(y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88584\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "[START] this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "if max_num_words is None:\n",
    "    max_num_words = len(imdb.get_word_index().items())\n",
    "\n",
    "print(max_num_words)\n",
    "\n",
    "print_decoded_sequence(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training label distribution\n",
    "\n",
    "plt.figure()\n",
    "sns.countplot(pd.DataFrame(y_train, columns=[\"class\"]), x=\"class\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"y train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test label distribution\n",
    "\n",
    "plt.figure()\n",
    "sns.countplot(pd.DataFrame(y_test, columns=[\"class\"]), x=\"class\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"y test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words distribution\n",
    "\n",
    "\n",
    "def visualize_sequence_distribution(x_train, x_test):\n",
    "    review_len_train = []\n",
    "    review_len_test = []\n",
    "    for i, j in zip(x_train, x_test):\n",
    "        review_len_train.append(len(i))\n",
    "        review_len_test.append(len(j))\n",
    "\n",
    "    print(\"min:\", min(review_len_train), \"max:\", max(review_len_train))\n",
    "    print(\"min:\", min(review_len_test), \"max:\", max(review_len_test))\n",
    "\n",
    "    sns.displot(review_len_train, rug_kws={\"alpha\": 0.3})\n",
    "    plt.xlabel(\"review length\")\n",
    "    plt.title(\"review train\")\n",
    "    sns.displot(review_len_test, rug_kws={\"alpha\": 0.3})\n",
    "    plt.xlabel(\"review length\")\n",
    "    plt.title(\"review test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sequence_distribution(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sequence_len = np.mean([len(seq) for seq in x_train])\n",
    "mean_sequence_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras's IMDB\n",
    "\n",
    "X data : 資料已經預處理過，包括\n",
    "* normalization => setting English stopwords\n",
    "* removing html strips and noise text\n",
    "* removing special characters\n",
    "* segmentation (斷詞)\n",
    "* removing stopwords\n",
    "* encoding (編碼)\n",
    "\n",
    "y data : 代表正向 (positive) 或負向 (negative) 的評論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使每個 sequence 有相同的長度\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_sequence_length, padding=\"pre\")\n",
    "x_test = pad_sequences(x_test, maxlen=max_sequence_length, padding=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data shape:\", x_train.shape)\n",
    "print(\"Training label shape:\", y_train.shape)\n",
    "print(\"Test data shape:\", x_test.shape)\n",
    "print(\"Test label shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sequence_distribution(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_decoded_sequence(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN mechanism\n",
    "\n",
    "Ref.\n",
    "\n",
    "[一文搞懂RNN（循环神经网络）基础篇](https://zhuanlan.zhihu.com/p/30844905)\n",
    "\n",
    "![rnn](./images/rnn.png)\n",
    "\n",
    "S<sub>t</sub> 代表神經元在 t 時刻上，同時接收\n",
    "* 輸入 X 與其輸入權重 U\n",
    "* 前一次 S 在 t-1 時刻的 S<sub>t-1</sub> 與其 W (S<sub>t-1</sub> --> S<sub>t</sub> 的權重)\n",
    "\n",
    "展開\n",
    "\n",
    "![rnn-expand](./images/rnn-expand.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mechanism\n",
    "\n",
    "Ref.\n",
    "\n",
    "[完全解析RNN, Seq2Seq, Attention注意力机制](https://zhuanlan.zhihu.com/p/51383402)\n",
    "\n",
    "[A simple overview of RNN, LSTM and Attention Mechanism](https://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b)\n",
    "\n",
    "[注意力機制 (Attention Mechanism) 的理解與實作](https://www.kaggle.com/code/lianghsunhuang/attention-mechanism)\n",
    "\n",
    "Attention 的架構\n",
    "\n",
    "![attention-mechanism](./images/attention-mechanism.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref.\n",
    "\n",
    "[Keras 中的循环神经网络 (RNN)](https://tensorflow.google.cn/guide/keras/rnn?hl=zh-cn)\n",
    "\n",
    "[Keras实现CNN、RNN（基于attention 的双向RNN）及两者的融合](https://blog.csdn.net/xwd18280820053/article/details/80060544)\n",
    "\n",
    "[用LSTM模型分類IMDB電影資料集評論](https://dysonma.github.io/2020/11/21/LSTM_IMDB/)\n",
    "\n",
    "very useful\n",
    "\n",
    "[Text classification with an RNN](https://www.tensorflow.org/text/tutorials/text_classification_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_activation = \"selu\" if not use_cuDNN else \"tanh\"\n",
    "\n",
    "model = keras.Sequential(name=\"BidirectionalRnn\")\n",
    "model.add(layers.Embedding(max_num_words, embedding_output_dim, input_length=max_sequence_length, mask_zero=True))\n",
    "model.add(layers.Bidirectional(layers.LSTM(64, activation=rnn_activation, return_sequences=True)))\n",
    "model.add(layers.Bidirectional(layers.LSTM(32, activation=rnn_activation, return_sequences=False)))\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build((None, max_sequence_length))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=False),\n",
    "    optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=num_epochs,\n",
    "    # verbose=0,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=validation_split,\n",
    "    validation_steps=validation_steps,\n",
    "    # callbacks=[TqdmCallback(verbose=0)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training:\n",
    "    plt.figure(figsize=[6, 4])\n",
    "    plt.plot(train_history.history[\"loss\"], \"black\", linewidth=2.0)\n",
    "    plt.plot(train_history.history[\"val_loss\"], \"green\", linewidth=2.0)\n",
    "    plt.legend([\"Training Loss\", \"Validation Loss\"], fontsize=14)\n",
    "    plt.xlabel(\"Epochs\", fontsize=10)\n",
    "    plt.ylabel(\"Loss\", fontsize=10)\n",
    "    plt.title(\"Loss Curves\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training:\n",
    "    plt.figure(figsize=[6, 4])\n",
    "    plt.plot(train_history.history[\"accuracy\"], \"black\", linewidth=2.0)\n",
    "    plt.plot(train_history.history[\"val_accuracy\"], \"blue\", linewidth=2.0)\n",
    "    plt.legend([\"Training Accuracy\", \"Validation Accuracy\"], fontsize=14)\n",
    "    plt.xlabel(\"Epochs\", fontsize=10)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=10)\n",
    "    plt.title(\"Accuracy Curves\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on training 7: found best index is 40\n",
    "\n",
    "np.argmin(train_history.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not training:\n",
    "    test_result = model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test result insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
